{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNy0FnmyAQG23Xrf5/yDMTQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Decoder-005/Quantum-Machine-Learning/blob/main/RandomForestClassical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRS-_KZ4k41J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: give a code for random forest with time complexity and accuracy using quantum machine learning\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "# Generate some sample data (replace with your actual data)\n",
        "X = np.random.rand(100, 10)  # 100 samples, 10 features\n",
        "y = np.random.randint(0, 2, 100)  # Binary classification\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust hyperparameters\n",
        "\n",
        "# Time the training process\n",
        "start_time = time.time()\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Random Forest Classifier\")\n",
        "print(\"Training Time:\", training_time, \"seconds\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "# Note:  This code does *not* use quantum computing.\n",
        "# Quantum machine learning is still in its early stages.\n",
        "# There aren't readily available, production-ready quantum algorithms that can directly replace classical algorithms like RandomForest for general-purpose classification tasks with significant performance gains.\n",
        "\n",
        "# To explore quantum machine learning in a research context, you would typically use quantum algorithms like Quantum Support Vector Machines (QSVM), Variational Quantum Classifiers (VQC), or quantum kernel methods.  These methods are integrated into frameworks like PennyLane, qiskit, or Cirq.\n",
        "\n",
        "# The time complexity of classical Random Forest is related to the number of trees, features, and samples.  Estimating its complexity for your specific data with specific parameters can require specialized profiling tools.\n",
        "# The time complexity of potential quantum machine learning algorithms depends on the specifics of the quantum algorithm (e.g., depth and width of the circuit) and is a subject of ongoing research."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUP9_kMopAGv",
        "outputId": "23d00c02-4edf-4538-b4b8-0d009e011fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier\n",
            "Training Time: 0.8334178924560547 seconds\n",
            "Accuracy: 0.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Load sample dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Randomized search for hyperparameter tuning\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 200),  # Number of trees\n",
        "    'max_depth': randint(5, 20),       # Depth of trees\n",
        "    'min_samples_split': randint(2, 10),  # Minimum samples required to split a node\n",
        "    'min_samples_leaf': randint(1, 10),   # Minimum samples required to be at a leaf node\n",
        "    'max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider for best split\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Randomized Search for Hyperparameter Tuning\n",
        "random_search = RandomizedSearchCV(\n",
        "    model, param_distributions=param_dist, n_iter=50, scoring='accuracy',\n",
        "    cv=5, random_state=42, n_jobs=-1, verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model with the best parameters\n",
        "random_search.fit(X_train, y_train)\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {random_search.best_params_}\")\n",
        "print(f\"Optimized Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29A4qMhrxzxh",
        "outputId": "766ec596-8d03-4887-8222-548197f5d348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Best Parameters: {'max_depth': 19, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 104}\n",
            "Optimized Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "115 fits failed out of a total of 250.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "91 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "24 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1107: UserWarning: One or more of the test scores are non-finite: [       nan 0.94166667        nan        nan        nan        nan\n",
            " 0.95              nan        nan        nan        nan 0.94166667\n",
            "        nan 0.94166667 0.95       0.94166667 0.94166667        nan\n",
            " 0.94166667        nan        nan 0.95              nan 0.94166667\n",
            "        nan        nan 0.95       0.95       0.94166667 0.94166667\n",
            "        nan 0.94166667        nan 0.94166667        nan 0.94166667\n",
            " 0.94166667 0.94166667        nan        nan 0.94166667 0.95\n",
            "        nan 0.94166667 0.94166667 0.94166667 0.94166667 0.94166667\n",
            "        nan 0.94166667]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Load sample dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Corrected parameter distribution\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 200),  # Number of trees\n",
        "    'max_depth': randint(5, 20),       # Depth of trees\n",
        "    'min_samples_split': randint(2, 10),  # Minimum samples required to split a node\n",
        "    'min_samples_leaf': randint(1, 10),   # Minimum samples required to be at a leaf node\n",
        "    'max_features': ['sqrt', 'log2', None]  # Valid options for max_features\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Randomized Search for Hyperparameter Tuning\n",
        "random_search = RandomizedSearchCV(\n",
        "    model, param_distributions=param_dist, n_iter=50, scoring='accuracy',\n",
        "    cv=5, random_state=42, n_jobs=-1, verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model with the best parameters\n",
        "random_search.fit(X_train, y_train)\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {random_search.best_params_}\")\n",
        "print(f\"Optimized Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcTrqfl3yhWv",
        "outputId": "e511151f-fd72-4a87-85e7-900ffb7d6f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 9, 'n_estimators': 180}\n",
            "Optimized Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import randint\n",
        "import time  # For measuring execution time\n",
        "\n",
        "# Load sample dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "start_time = time.time()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "split_time = time.time()\n",
        "\n",
        "# Corrected parameter distribution\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 200),  # Number of trees\n",
        "    'max_depth': randint(5, 20),       # Depth of trees\n",
        "    'min_samples_split': randint(2, 10),  # Minimum samples required to split a node\n",
        "    'min_samples_leaf': randint(1, 10),   # Minimum samples required to be at a leaf node\n",
        "    'max_features': ['sqrt', 'log2', None]  # Valid options for max_features\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Randomized Search for Hyperparameter Tuning\n",
        "random_search = RandomizedSearchCV(\n",
        "    model, param_distributions=param_dist, n_iter=50, scoring='accuracy',\n",
        "    cv=5, random_state=42, n_jobs=-1, verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model with the best parameters\n",
        "search_start_time = time.time()\n",
        "random_search.fit(X_train, y_train)\n",
        "search_end_time = time.time()\n",
        "\n",
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "predict_start_time = time.time()\n",
        "y_pred = best_model.predict(X_test)\n",
        "predict_end_time = time.time()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {random_search.best_params_}\")\n",
        "print(f\"Optimized Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Data Split Time: {split_time - start_time:.4f} seconds\")\n",
        "print(f\"Randomized Search Time: {search_end_time - search_start_time:.4f} seconds\")\n",
        "print(f\"Prediction Time: {predict_end_time - predict_start_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zj8IAQuyhct",
        "outputId": "186d2905-3372-442f-91a8-273578c5dec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Best Parameters: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 9, 'n_estimators': 180}\n",
            "Optimized Accuracy: 1.00\n",
            "Data Split Time: 0.0015 seconds\n",
            "Randomized Search Time: 53.9714 seconds\n",
            "Prediction Time: 0.0206 seconds\n"
          ]
        }
      ]
    }
  ]
}